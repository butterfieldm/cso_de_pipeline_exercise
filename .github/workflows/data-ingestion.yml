name: Data Ingestion Trigger

on:
  push:
    branches:
      - main
    paths:
      - 'data/**'
      - 'schemas/**'
  workflow_dispatch:

jobs:
  ingest:
    runs-on: ubuntu-latest
    env:
      PROJECT_ID: cso-deng-pipeline
      REGION: us-central1
      BUCKET_NAME: cso-exercise-ingestion-raw
      DATAFLOW_TEMPLATE_PATH: gs://cso-exercise-ingestion-raw/templates/cso-dataflow-template.json

    steps:
      - uses: actions/checkout@v4

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_PROJECT_KEY }}

      - name: Set up Google Cloud CLI
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.PROJECT_ID }}

      - name: Get Changed CSV Files
        id: changed
        run: |
            git fetch --depth=2 origin main || true
        
            if git rev-parse HEAD^ 2>/dev/null; then
              echo "Found previous commit, checking diff..."
              CHANGED_FILES=$(git diff --name-only HEAD^ HEAD | grep '^data/.*\.csv$' || true)
            else
              echo "No previous commit found (manual run or shallow clone). Using all CSVs in data/."
              CHANGED_FILES=$(find data -type f -name "*.csv")
            fi
        
            if [ -z "$CHANGED_FILES" ]; then
              echo "No CSV changes found."
              echo "changed_files=" >> $GITHUB_OUTPUT
              exit 0
            fi
        
            echo "Changed CSVs:"
            echo "$CHANGED_FILES"
        
            # Convert to comma-separated list
            CHANGED_CSVS=$(echo "$CHANGED_FILES" | paste -sd "," -)
            echo "changed_files=$CHANGED_CSVS" >> $GITHUB_OUTPUT
        
      - name: List changed files
        run: |
          echo "Changed files: ${{ steps.changed.outputs.changed_files }}"
        
      - name: Upload Changed CSV Files to GCS
        if: steps.changed.outputs.changed_files != ''
        run: |
            FILE_LIST="${{ steps.changed.outputs.changed_files }}"
            FILE_LIST_SC="${FILE_LIST//,/;}"
            IFS=';' read -ra FILES <<< "$FILE_LIST_SC"
            for file in "${FILES[@]}"; do
              echo "Uploading data file: $file"
              gsutil cp "$file" gs://${{ env.BUCKET_NAME }}/$file
              
              # Extract filename without path and extension
              fname=$(basename "$file" .csv)
              
              # Upload corresponding schema file if exists
              schema_file="schemas/${fname}_schema.json"
              if [ -f "$schema_file" ]; then
                echo "Uploading schema file: $schema_file"
                gsutil cp "$schema_file" gs://${{ env.BUCKET_NAME }}/schemas/
              else
                echo "Schema file $schema_file not found, skipping."
              fi
            done
        

      - name: Upload mapping config to GCS
        run: |
            gsutil cp mappings.yaml gs://${{ env.BUCKET_NAME }}/config/mappings.yaml
        
      - name: Trigger Dataflow Job for Changed CSVs
        if: steps.changed.outputs.changed_files != ''
        run: |
            FILE_LIST="${{ steps.changed.outputs.changed_files }}"
            echo "$FILE_LIST"
           
            FILE_LIST_SC="${FILE_LIST//,/;}"
            echo "Changed delimeter: $FILE_LIST_SC"

            PARAMS="data_bucket=${{ env.BUCKET_NAME }},schema_bucket=${{ env.BUCKET_NAME }},data_prefix=data/,project=${{ env.PROJECT_ID }},changed_files=${FILE_LIST_SC}"
            
            echo "Final --parameters string: $PARAMS"
        
            gcloud dataflow flex-template run "run-ingestion-$(date +%s)" \
                --project=${{ env.PROJECT_ID }} \
                --region=${{ env.REGION }} \
                --template-file-gcs-location=${{ env.DATAFLOW_TEMPLATE_PATH }} \
                --parameters="$PARAMS"
            