name: Data Ingestion Trigger

on:
  push:
    branches:
      - main
    paths:
      - 'data/**'
      - 'schemas/**'
  workflow_dispatch:

jobs:
  ingest:
    runs-on: ubuntu-latest
    env:
      PROJECT_ID: cso-deng-pipeline
      REGION: europe-west2
      BUCKET_NAME: cso-exercise-ingestion-raw
      DATAFLOW_TEMPLATE_PATH: gs://cso-exercise-ingestion-raw/templates/cso-dataflow-template.json

    steps:
      - uses: actions/checkout@v4

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_PROJECT_KEY }}

      - name: Set up Google Cloud CLI
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.PROJECT_ID }}

      - name: Upload Schema & CSV
        run: |
          gsutil cp schemas/* gs://${{ env.BUCKET_NAME }}/schemas/
          gsutil cp data/* gs://${{ env.BUCKET_NAME }}/data/

      - name: Trigger Dataflow Jobs for all CSV files
        run: |
          for csvfile in data/*.csv; do
            base=$(basename "$csvfile" .csv)
            schemafile="schemas/${base}_schema.json"
            if [[ -f "$schemafile" ]]; then
              echo "Launching Dataflow for $csvfile with schema $schemafile"
              gcloud dataflow flex-template run "run-${base}-$(date +%s)" \
                --project=${{ env.PROJECT_ID }} \
                --region=${{ env.REGION }} \
                --template-file-gcs-location=${{ env.DATAFLOW_TEMPLATE_PATH }} \
                --parameters inputFile=gs://${{ env.BUCKET_NAME }}/data/${base}.csv,schemaFile=gs://${{ env.BUCKET_NAME }}/schemas/${base}_schema.json
            else
              echo "No matching schema for $csvfile, skipping."
            fi
          done
